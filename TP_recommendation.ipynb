{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TP Recommendation\n",
    "Loïc Herbelot\n",
    "\n",
    "### Question 1.1 : Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def load_movielens(filename, minidata=False):\n",
    "    \"\"\"\n",
    "    Cette fonction lit le fichier filename de la base de donnees\n",
    "    Movielens, par exemple \n",
    "    filename = '~/datasets/ml-100k/u.data'\n",
    "    Elle retourne \n",
    "    R : une matrice utilisateur-item contenant les scores\n",
    "    mask : une matrice valant 1 si il y a un score et 0 sinon\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename, dtype=int)\n",
    "\n",
    "    R = sparse.coo_matrix((data[:, 2], (data[:, 0]-1, data[:, 1]-1)),\n",
    "                          dtype=float)\n",
    "    R = R.toarray()  # not optimized for big data\n",
    "\n",
    "    # code la fonction 1_K\n",
    "    mask = sparse.coo_matrix((np.ones(data[:, 2].shape),\n",
    "                              (data[:, 0]-1, data[:, 1]-1)), dtype=bool )\n",
    "    mask = mask.toarray()  # not optimized for big data\n",
    "\n",
    "    if minidata is True:\n",
    "        R = R[0:100, 0:200].copy()\n",
    "        mask = mask[0:100, 0:200].copy()\n",
    "\n",
    "    return R, mask\n",
    "\n",
    "\n",
    "def objective(P, Q0, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme simplifie.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q0 : une matrice de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q0.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q0 ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = -Q0.T.dot(tmp) + rho*P\n",
    "\n",
    "    return val, grad_P\n",
    "\n",
    "\n",
    "def total_objective(P, Q, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme complet.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q : la variable matricielle de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    grad_Q : le gradient par rapport a Q\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = 0  # todo\n",
    "\n",
    "    grad_Q = 0  # todo\n",
    "\n",
    "    return val, grad_P, grad_Q\n",
    "\n",
    "\n",
    "def total_objective_vectorized(PQvec, R, mask, rho):\n",
    "    \"\"\"\n",
    "    Vectorisation de la fonction precedente de maniere a ne pas\n",
    "    recoder la fonction gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # reconstruction de P et Q\n",
    "    n_items = R.shape[1]\n",
    "    n_users = R.shape[0]\n",
    "    F = PQvec.shape[0] / (n_items + n_users)\n",
    "    Pvec = PQvec[0:n_items*F]\n",
    "    Qvec = PQvec[n_items*F:]\n",
    "    P = np.reshape(Pvec, (F, n_items))\n",
    "    Q = np.reshape(Qvec, (n_users, F))\n",
    "\n",
    "    val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "    return val, np.concatenate([grad_P.ravel(), grad_Q.ravel()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de R: (943, 1682)\n",
      "Taille de mask: (943, 1682)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename = \"ml-100k/u.data\"\n",
    "\n",
    "#sans \"minidata\"\n",
    "R, mask = load_movielens(filename)\n",
    "print(\"Taille de R:\", R.shape)\n",
    "print(\"Taille de mask:\", mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de R_mini: (100, 200)\n",
      "Taille de mask_mini: (100, 200)\n"
     ]
    }
   ],
   "source": [
    "#avec \"minidata\"\n",
    "R_mini, mask_mini = load_movielens(filename, minidata=True)\n",
    "print(\"Taille de R_mini:\", R_mini.shape)\n",
    "print(\"Taille de mask_mini:\", mask_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fonction de `minidata` :\n",
    "Comme on peut le voir dans le code source, l'argument `minidata` sert à indiquer si l'on veut une plus petite partie des informations, seulement 100 lignes et 200 colonnes.\n",
    "\n",
    "### Question 1.2 : Informations sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du fichier d'info : \n",
      "943 users\n",
      "1682 items\n",
      "100000 ratings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"ml-100k/u.info\") as f:\n",
    "    print(\"Contenu du fichier d'info : \\n\"+ f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On voit qu'on a donc :\n",
    " * 943 utilisateurs\n",
    " * 1682 films\n",
    " * Pour un total de 100 000 notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 1.3 : Étude de la fonction objectif\n",
    "\n",
    "Ici j'ai tracé la fonction avec deux variables :\n",
    "![Graphe de la fonction objectif avec deux variables](plot_objective_2_var.png)\n",
    "\n",
    "\n",
    "On voit donc que la fonction n'est pas convexe.\n",
    "\n",
    "Le gradient de f est :\n",
    "\n",
    "$\\nabla f(P,Q) = (Q^T (1_K ⋅ (QP-R)) + \\rho P, 1_K ⋅ (QP-R)P^T + \\rho Q)$\n",
    "\n",
    "Le gradient de $f$ n'est pas lipschitzien, car ses dérivées partielles ne sont pas bornées.\n",
    "\n",
    "Exemple en dimension 1 :\n",
    "$f(x,y) = \\frac 1 2 (r- xy)^2 + \\frac \\rho 2 (x^2 + y^2) $\n",
    "\n",
    "$\\frac{df}{dx}(x) = x(y^2 + \\rho) - yr$ et n'est pas bornée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2.1 :\n",
    "\n",
    "La fonction $g$ est convexe.\n",
    "\n",
    "Son gradient vaut : $\\nabla g(P) = -{Q^0}^T(1_K ⋅ (R - Q^0*P)) + \\rho P $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2.2 : Calcul du gradient de $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérification sautée.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "# Initialisation des variables :\n",
    "rho = 0.2\n",
    "P = np.random.random((7, 1682))*100\n",
    "\n",
    "# Fonctions utilisées pour la vérification du gradient :\n",
    "\n",
    "# def g(P):\n",
    "#     \"\"\" Renvoie la sortie de la fonction objectif.\n",
    "#     :param P: un vecteur ligne de 7*1682 éléments\n",
    "#     :return : la sortie de la fonction objectif.\"\"\"\n",
    "#     P = P.reshape((7, 1682))\n",
    "#     return objective(P, Q0, R, mask, rho)[0]\n",
    "\n",
    "# def grad_g(P):\n",
    "#     \"\"\"Renvoie le gradient de la fonction objectif au point P.\n",
    "#     :param P: un vecteur ligne de 7*1682 éléments\n",
    "#     :return: le gradient sous forme de vecteur ligne.\"\"\"\n",
    "#     P = P.reshape((7, 1682))\n",
    "#     return objective(P, Q0, R, mask, rho)[1].ravel()\n",
    "\n",
    "# Activer/Désactiver la vérification du gradient :\n",
    "verif = False # Choix de l'utilisateur\n",
    "if verif :\n",
    "    # Attention, calcul très long :\n",
    "    print(\"Erreur sur le gradient :\")\n",
    "    for i in range(1):\n",
    "        Q0 = np.random.random((943, 7))\n",
    "        P = np.random.random((7, 1682))\n",
    "        print(\"Vérification n°%d :\" % i)\n",
    "        print(check_grad(lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[0],\n",
    "                         lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[1].ravel(),\n",
    "                         P.ravel()))\n",
    "else :\n",
    "    print(\"Vérification sautée.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2.3 : Gradient à pas constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def gradient(objective, P0, gamma=1, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise une fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param g:       La fonction à minimiser\n",
    "    :param P0:      Le point de départ\n",
    "    :param gamma:   Le pas\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point, et son gradient\n",
    "    \"\"\"\n",
    "    #Q0 est la matrice des 7 vecteurs singuliers à gauche de R\n",
    "    # (|C| = 7)\n",
    "    rho = 0.2\n",
    "    Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "    # Pour avoir tout le temps le même Q0 :\n",
    "    for i in range(Q0.shape[1]):\n",
    "        Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "#     print(\"Q0:\",Q0[:3,:3])\n",
    "#     print(\"R:\",R[:3,:3])\n",
    "#     print(\"mask:\",mask[:3,:3])\n",
    "    x = P0\n",
    "    grad = objective(x, Q0, R, mask, rho)[1]\n",
    "    i = 0\n",
    "#     L = rho + np.linalg.norm(Q0.T.dot(Q0), 'fro')\n",
    "#     gamma = 1/L\n",
    "    while i < n_iter and np.linalg.norm(grad, 'fro') > epsilon:\n",
    "        x -= gamma * grad\n",
    "        val, grad = objective(x, Q0, R, mask, rho)\n",
    "        if i%10 == 0 and verbose:\n",
    "            print(\"Iteration n°%5d, g(x) = %E\" % (i, val))\n",
    "        i += 1\n",
    "    if verbose:\n",
    "        print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "    return x, val, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2.4 : Démonstration de la fonction `gradient` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration n°    0, g(x) = 2.510641E+08\n",
      "Iteration n°   10, g(x) = 1.513685E+06\n",
      "Iteration n°   20, g(x) = 3.086310E+05\n",
      "Iteration n°   30, g(x) = 2.979350E+05\n",
      "Iteration n°   40, g(x) = 2.978283E+05\n",
      "Valeur maximale du gradient en ce point :  0.00360485130878\n",
      "Minimum trouvé : 2.978272E+05\n",
      "\n",
      "-- Retrouvons les valeurs :\n",
      "Minimum trouvé : 2.978272E+05\n",
      "Valeur maximale du gradient en ce point :  0.00360485130878\n",
      "\n",
      "-- 10 valeurs au hasard :\n",
      "g(P) = 6.232252E+08\n",
      "g(P) = 6.221720E+08\n",
      "g(P) = 6.216858E+08\n",
      "g(P) = 6.295080E+10\n",
      "g(P) = 6.226462E+10\n",
      "g(P) = 6.276031E+08\n",
      "g(P) = 6.360091E+10\n",
      "g(P) = 6.216095E+06\n",
      "g(P) = 6.119699E+08\n",
      "g(P) = 6.247651E+08\n"
     ]
    }
   ],
   "source": [
    "# Trouvons le P qui minimise g :\n",
    "P_min, val, grad = gradient(objective, np.random.random((7, 1682))*1000, 1, 0.1, n_iter=1000)\n",
    "print(\"Minimum trouvé : %E\" % val)\n",
    "\n",
    "# Vérification :\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "# print(\"Q0:\",Q0[:3,:3])\n",
    "# print(\"R:\",R[:3,:3])\n",
    "# print(\"mask:\",mask[:3,:3])\n",
    "val_v, grad_v = objective(P_min, Q0, R, mask, rho)\n",
    "print(\"\\n-- Retrouvons les valeurs :\")\n",
    "print(\"Minimum trouvé : %E\" % val_v)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad_v))\n",
    "\n",
    "\n",
    "# Essayons 10 valeurs au hasard pour vérifier la pertinence du résultat :\n",
    "print(\"\\n-- 10 valeurs au hasard :\")\n",
    "for i in range(10):\n",
    "    # J'évite de prendre des matrices \"proches\" de 0, \n",
    "    # sinon on ne pourrait pas voir comment se comporte g.\n",
    "    # Je multiplie une matrice \"random\" par 10^[un entier au hasard]\n",
    "    exp = np.random.randint(-1, 5)\n",
    "    P = np.random.random((7, 1682))*10**exp\n",
    "    rand_val = objective(P, Q0, R, mask, rho)[0]\n",
    "    if rand_val < val:\n",
    "        print(\"Nouveau minimum trouvé :\")\n",
    "    print(\"g(P) = %E\" % (rand_val))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.1 : Recherche linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration n°    0, g(x) = 6.223391E+05\n",
      "Iteration n°  100, g(x) = 2.978949E+05\n",
      "()\n",
      "(7, 1682)\n",
      "Minimum trouvé : 2.978291E+05\n",
      "Valeur maximale du gradient en ce point :  0.0389871818733\n",
      "Fin de la fonction lin_gradient.\n",
      "\n",
      "Début de la vérification\n",
      "\n",
      "-- Valeurs renvoyées par la fonction :\n",
      "Minimum trouvé : 2.978291E+05\n",
      "Valeur maximale du gradient en ce point :  0.0389871818733\n",
      "\n",
      "-- Retrouvons les valeurs objectif : \n",
      "Minimum trouvé : 2.978291E+05\n",
      "Valeur maximale du gradient en ce point :  0.0389871818733\n",
      "\n",
      "-- 10 valeurs au hasard :\n",
      "g(P) = 6.674244E+05\n",
      "g(P) = 6.649596E+05\n",
      "g(P) = 6.703088E+05\n",
      "g(P) = 6.730589E+05\n",
      "g(P) = 6.271316E+06\n",
      "g(P) = 6.699229E+05\n",
      "g(P) = 6.789845E+05\n",
      "g(P) = 6.313953E+10\n",
      "g(P) = 6.271528E+10\n",
      "g(P) = 6.790300E+05\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def lin_gradient(g, P0, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise la fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param g:       La fonction à minimiser\n",
    "    :param P0:      Le point de départ\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point, et son gradient\n",
    "    \"\"\"\n",
    "    #Q0 est la matrice des 7 vecteurs singuliers à gauche de R\n",
    "    # (|C| = 7)\n",
    "    rho = 0.2\n",
    "    Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "    # Pour avoir tout le temps le même Q0 :\n",
    "    for i in range(Q0.shape[1]):\n",
    "        Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "    x = P0\n",
    "    val, grad = objective(x, Q0, R, mask, rho)\n",
    "    i = 0\n",
    "    gamma_values = np.linspace(0.1, 10, 100)\n",
    "#     L = rho + np.linalg.norm(Q0.T.dot(Q0), 'fro')\n",
    "#     gamma = 1/L\n",
    "    while i < n_iter and np.linalg.norm(grad, 'fro') > epsilon:\n",
    "        g = lambda x:objective(x, Q0, R, mask, rho)[0]\n",
    "        id_argmin = np.argmin(lambda gamma: g(x - gamma*grad))\n",
    "        x -= gamma_values[id_argmin] * grad\n",
    "        val, grad = objective(x, Q0, R, mask, rho)\n",
    "        # Affichage des résultats :\n",
    "        if i%100 == 0 and verbose:\n",
    "            print(\"Iteration n°%5d, g(x) = %E\" % (i, val))\n",
    "        i += 1\n",
    "    if verbose:\n",
    "        print(val.shape)\n",
    "        print(grad.shape)\n",
    "        print(\"Minimum trouvé : %E\" % val)\n",
    "        print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "    return x, val, grad\n",
    "\n",
    "\n",
    "# Trouvons le P qui minimise g :\n",
    "P_min, val, grad = lin_gradient(g, np.random.random((7, 1682)), 1, n_iter=1000)\n",
    "print(\"Fin de la fonction lin_gradient.\\n\")\n",
    "print(\"Début de la vérification\")\n",
    "print(\"\\n-- Valeurs renvoyées par la fonction :\")\n",
    "print(\"Minimum trouvé : %E\" % val)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "print(\"\\n-- Retrouvons les valeurs objectif : \")\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "val, grad = objective(P_min, Q0, R, mask, rho)\n",
    "print(\"Minimum trouvé : %E\" % val)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad))\n",
    "\n",
    "# Essayons 10 valeurs au hasard pour vérifier la pertinence du résultat :\n",
    "print(\"\\n-- 10 valeurs au hasard :\")\n",
    "for i in range(10):\n",
    "    # J'évite de prendre des matrices \"proches\" de 0, \n",
    "    # sinon on ne pourrait pas voir comment se comporte g.\n",
    "    # Je multiplie une matrice \"random\" par 10^[un entier au hasard]\n",
    "    exp = np.random.randint(-1, 5)\n",
    "    P = np.random.random((7, 1682))*10**exp\n",
    "    rand_val = objective(P, Q0, R, mask, rho)[0]\n",
    "    if rand_val < val:\n",
    "        print(\"Nouveau minimum trouvé :\")\n",
    "    print(\"g(P) = %E\" % (rand_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.2 : Gradient conjugué :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 297829.056960\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Fin de la fonction conj_gradient.\n",
      "\n",
      "\n",
      "-- Vérification :\n",
      "Valeur en P_tilde : 2.978291E+05\n",
      "Valeur maximale du gradient en ce point :  0.0320287853276\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def conj_gradient(g, epsilon=1, n_iter=1000, verbose=True):\n",
    "    \"\"\" Minimise une fonction objectif par la méthode du gradient à pas constant.\n",
    "    :param g:       La fonction à minimiser\n",
    "    :param P0:      Le point de départ\n",
    "    :param epsilon: Critère d'arrêt : norme du gradient inférieure à epsilon\n",
    "    :param n_iter:  Le nombre maximum d'itérations\n",
    "    \n",
    "    :return: Le point qui minimise la fonction, la valeur de la fonction en ce point, \n",
    "    et son gradient\n",
    "    \"\"\"\n",
    "    #Q0 est la matrice des 7 vecteurs singuliers à gauche de R\n",
    "    # (|C| = 7)\n",
    "    rho = 0.2\n",
    "    Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "    # Pour avoir tout le temps le même Q0 :\n",
    "    for i in range(Q0.shape[1]):\n",
    "        Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "        \n",
    "    approx_P, val, grad = lin_gradient(f, np.random.random((7, 1682)), 1, \n",
    "                                       n_iter=10, verbose=False)\n",
    "    g = lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[0]\n",
    "    g_prime = lambda P:objective(P.reshape((7, 1682)), Q0, R, mask, rho)[1].ravel()\n",
    "    return fmin_cg(g, P.ravel(), g_prime, \n",
    "                   gtol=epsilon, maxiter=n_iter, disp=verbose).reshape((7, 1682))\n",
    "\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "P_tilde = conj_gradient(g,1)\n",
    "print(\"Fin de la fonction conj_gradient.\\n\")\n",
    "print(\"\\n-- Vérification :\")\n",
    "val, grad = objective(P_tilde, Q0, R,mask, rho)\n",
    "print(\"Valeur en P_tilde : %E\" % val)\n",
    "print(\"Valeur maximale du gradient en ce point : \",np.max(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3.3 : Comparaison des algos :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode utilisée : 'constant_step'....................\n",
      "Méthode utilisée : 'linear'....................\n",
      "Méthode utilisée : 'conjugate'....................\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "mult = [10.**e for e in np.random.randint(-1, 5, size=20)]\n",
    "rand_mat = [np.random.random((7, 1682)) * mul for mul in mult]\n",
    "\n",
    "methods = {\"constant_step\":{\"scores\":[],\"times\":[], \"func\":gradient},\n",
    "           \"linear\":       {\"scores\":[],\"times\":[], \"func\":lin_gradient},\n",
    "           \"conjugate\":       {\"scores\":[],\"times\":[], \"func\":conj_gradient}}\n",
    "\n",
    "rho = 0.2\n",
    "Q0, s, vt = svds(R, min(7, min(R.shape)))\n",
    "# Pour avoir tout le temps le même Q0 :\n",
    "for i in range(Q0.shape[1]):\n",
    "    Q0[...,i] *= np.sign(Q0[0,i]) \n",
    "    \n",
    "for meth in (\"constant_step\", \"linear\"):\n",
    "    print(\"Méthode utilisée : '%s'\" % meth, end='')\n",
    "    for mat in rand_mat:\n",
    "        print('.', end='')\n",
    "        t1 = time.clock()\n",
    "        P = methods[meth][\"func\"](objective, mat, verbose=False)[0]\n",
    "        methods[meth][\"times\"].append(time.clock() - t1)\n",
    "        methods[meth][\"scores\"].append(objective(P, Q0, R, mask, rho)[0])\n",
    "    print()\n",
    "    \n",
    "meth = \"conjugate\"\n",
    "print(\"Méthode utilisée : '%s'\" % meth, end='')\n",
    "for mat in rand_mat:\n",
    "    print('.', end='')\n",
    "    t1 = time.clock()\n",
    "    P = methods[meth][\"func\"](objective, verbose=False)\n",
    "    methods[meth][\"times\"].append(time.clock() - t1)\n",
    "    methods[meth][\"scores\"].append(objective(P, Q0, R, mask, rho)[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61dbdc0160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for meth in methods.keys():\n",
    "    plt.scatter(methods[meth][\"times\"], methods[meth][\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
